{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV_TgfFyygun"
      },
      "source": [
        "# Colab Notebook for Neuromorphic Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8yvSeE2zezt"
      },
      "source": [
        "## Execution functions\n",
        "- Includes main function, import statements and additional functions required for the program execution (such as accuracy calculator, testing/validation functions, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUdgvZojyczF"
      },
      "outputs": [],
      "source": [
        "import pickle as cPickle, gzip, numpy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as td\n",
        "import torch.nn.functional as F\n",
        "import time, random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import logging\n",
        "logging.basicConfig(filename='std.log', filemode='w', level=logging.DEBUG)\n",
        "\n",
        "# Setting random seed for reproducability\n",
        "manualSeed = 5000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def set_seed():\n",
        "  if device == 'cuda':\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "  torch.manual_seed(manualSeed)\n",
        "  np.random.seed(manualSeed)\n",
        "  random.seed(manualSeed)\n",
        "\n",
        "sparsity = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgm3UgeJyp6g"
      },
      "outputs": [],
      "source": [
        "# Loading the datasets\n",
        "def data_processing(cnn):\n",
        "  f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "  train_set, valid_set, test_set = cPickle.load(f, encoding='latin1') \n",
        "  f.close()\n",
        "  # Converting dataset from numpy to Tensor\n",
        "  train_t = torch.from_numpy(train_set[0])\n",
        "  valid_t = torch.from_numpy(valid_set[0])\n",
        "  test_t  = torch.from_numpy(test_set[0])\n",
        "  # Converting labels to Tensor\n",
        "  train_label= torch.from_numpy(train_set[1])\n",
        "  valid_label= torch.from_numpy(valid_set[1])\n",
        "  test_label = torch.from_numpy(test_set[1])\n",
        "  if cnn:\n",
        "    # resize and normalize\n",
        "    train_t = train_t.reshape((train_t.shape[0], 1, 28, 28))\n",
        "    valid_t = valid_t.reshape((valid_t.shape[0], 1, 28, 28))\n",
        "    test_t = test_t.reshape((test_t.shape[0], 1, 28, 28))\n",
        "    input_shape = (1, 28, 28)\n",
        "  # Wrapping it\n",
        "  trainset = TensorDataset(train_t, train_label)\n",
        "  validset = TensorDataset(valid_t, valid_label)\n",
        "  testset = TensorDataset(test_t, test_label)\n",
        "\n",
        "  return trainset, validset, testset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl-eboIdywii"
      },
      "outputs": [],
      "source": [
        "# Loss Curve grapher\n",
        "def loss_curve(train_loss, val_loss, y_range, name):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(name)\n",
        "  plt.plot(train_loss, label=\"Training Loss\")\n",
        "  plt.plot(val_loss, label=\"Validation Loss\")\n",
        "  plt.legend()\n",
        "  plt.savefig(name+\".png\", bbox_inches='tight', pad_inches=0.1)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eFEGozcyywu"
      },
      "outputs": [],
      "source": [
        "# Accuracy Calculator\n",
        "def acc_calc(pred, actual):\n",
        "  best = pred.argmax(1)\n",
        "  comp = best.eq(actual.view_as(best)).float().sum()\n",
        "  return comp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0g9KY21y0kS"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_loop(data_loader, NN_model, loss_fn, optimizer):\n",
        "  size = len(data_loader.dataset)\n",
        "  loss_val = 0\n",
        "  loss, acc = 0,0\n",
        "  NN_model.train()\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      X = X.cuda()\n",
        "      y = y.cuda()\n",
        "    # Compute prediction and loss\n",
        "    pred = NN_model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "    \n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Accuracy\n",
        "    acc = acc+acc_calc(nn.Softmax(dim=1)(pred), y)\n",
        "    loss_val = loss_val+loss.item()\n",
        "  return loss_val/size, (acc/size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4eWjGL2y2B8"
      },
      "outputs": [],
      "source": [
        "# Function for validation and testing\n",
        "def eval_loop(data_loader, model, loss_fn):\n",
        "  size = len(data_loader.dataset)\n",
        "  loss, acc, loss_val = 0, 0, 0\n",
        "  model.eval()\n",
        "  torch.no_grad() \n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      X = X.cuda()\n",
        "      y = y.cuda()\n",
        "    \n",
        "    target = model(X)\n",
        "    loss = loss_fn(target, y).item()\n",
        "    # Accuracy\n",
        "    acc = acc+acc_calc(nn.Softmax(dim=1)(target), y)\n",
        "    loss_val = loss_val+loss\n",
        "\n",
        "  return loss_val/size, (acc/size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omNbNQlpy6DL"
      },
      "outputs": [],
      "source": [
        "# Primary function for execution of the neural networks\n",
        "def nn_run(nn_model, trainset, validset, testset, batch, learn_rate, epoch_max, mom, model, decay):\n",
        "  \n",
        "  # Loading datasets\n",
        "  train_loader = td.DataLoader(trainset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "  valid_loader = td.DataLoader(validset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "  test_loader  = td.DataLoader(testset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "  print(test_loader)\n",
        "\n",
        "  # Loading neural network model to device\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(nn_model.parameters(), lr=learn_rate, momentum=mom, weight_decay=decay)\n",
        "  # optimizer = torch.optim.SGD(nn_model.parameters(), lr=learn_rate, weight_decay=decay)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
        "\n",
        "  print(f'\\n=============={model}==============')\n",
        "  print(f'Starting training using {device} device...')\n",
        "  timer = time.perf_counter()\n",
        "  valid_loss = 1\n",
        "  valid_loss_list = []\n",
        "  train_loss_list = []\n",
        "  valid_acc_list = []\n",
        "  train_acc_list = []\n",
        "  for epoch in range(epoch_max):\n",
        "    train_loss, train_acc = train_loop(train_loader, nn_model, loss_fn, optimizer)\n",
        "    loss_temp, acc_temp = eval_loop(valid_loader, nn_model, loss_fn)\n",
        "\n",
        "    valid_loss_list.append(loss_temp)\n",
        "    train_loss_list.append(train_loss)\n",
        "    valid_acc_list.append(acc_temp.cpu())\n",
        "    train_acc_list.append(train_acc.cpu())\n",
        "    \n",
        "    if loss_temp < valid_loss:\n",
        "      valid_acc = acc_temp\n",
        "      valid_loss = loss_temp\n",
        "    print(\"\", end=f\"\\rEpoch {epoch+1} ({round((epoch+1)/epoch_max * 100, 2)}%) complete. Runtime: {round(time.perf_counter()-timer, 2)}s      \")\n",
        "\n",
        "\n",
        "  timer = round(time.perf_counter()-timer, 2)\n",
        "\n",
        "  print('\\n\\n--------Training complete.--------')\n",
        "  print(f'Model                    : {model}')\n",
        "  print(f'Training loss            : {train_loss:>7f}')\n",
        "  print(f'Batch Size               : {batch}')\n",
        "  print(f'Training accuracy        : {np.round(train_acc.cpu().detach().numpy()*100, 3)}%') if device == 'cuda' else print(f'Training accuracy        : {np.round(train_acc.detach().numpy()*100, 3)}%')\n",
        "  print(f'Training time            : {int(timer/60)}m {int(timer%60)}s')\n",
        "  print(f'Best validation loss     : {valid_loss:>7f}')\n",
        "  print(f'Best validation accuracy : {np.round(valid_acc.cpu().detach().numpy()*100, 3)}%') if device == 'cuda' else print(f'Best validation accuracy : {np.round(valid_acc.detach().numpy()*100, 3)}%')\n",
        "  print(f'Number of epochs         : {epoch_max}')\n",
        "  print(f'Learning rate            : {learn_rate}')\n",
        "  print(f'Training dataset size    : {len(train_loader.dataset)}')\n",
        "  print(f'Device                   : {torch.cuda.get_device_name(device)}')  if device=='cuda' else print(f'Device                   : {device}')\n",
        "  print('----------------------------------')\n",
        "  \n",
        "  #=========TESTING=========\n",
        "  print(f'\\nTesting using {device} device.')\n",
        "  timer = time.perf_counter()\n",
        "  test_loss, test_acc = eval_loop(test_loader, nn_model, loss_fn)\n",
        "  timer = round(time.perf_counter()-timer, 2)\n",
        "\n",
        "  print('--------Test complete.--------')\n",
        "  print(f'Model       : {model}')\n",
        "  print(f'Device      : {device}')\n",
        "  print(f'Loss        : {test_loss:>7f}')\n",
        "  print(f'Accuracy    : {np.round(test_acc.cpu().detach().numpy()*100, 3)}%') if device == 'cuda' else print(f'Accuracy    : {np.round(test_acc.detach().numpy()*100, 3)}%')\n",
        "  print(f'Runtime     : {timer}s')\n",
        "  print(f'Dataset size: {len(test_loader.dataset)}')\n",
        "  print('------------------------------\\n')\n",
        "  \n",
        "  loss_curve(train_loss_list, valid_loss_list, epoch_max, model+\" Loss Curve\")\n",
        "  loss_curve(train_acc_list, valid_acc_list, epoch_max, model+\" Accuracy Curve\")\n",
        "\n",
        "  # Returning test accuracy to main function\n",
        "  return np.round(test_acc.cpu().detach().numpy()*100, 3) if device == 'cuda' else np.round(test_acc.detach().numpy()*100, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjohW-GQy6zb"
      },
      "source": [
        "### Neural Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw7RCCN3zAnw"
      },
      "outputs": [],
      "source": [
        "# Defining the convolutional neural network (CNN)\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    first_layer_features = 16\n",
        "    second_layer_features = 32\n",
        "    first_layer_sparsity = 0\n",
        "    second_layer_sparsity = 0.5\n",
        "    wbit = 4\n",
        "    abit = 4\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "      # Defining first custom 2D convolution layer\n",
        "      QConv2d(1, first_layer_features, kernel_size=3, stride=1, padding=1, wbit=wbit, abit=abit, sparsity_fraction=first_layer_sparsity),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      # Defining second custom 2D convolution layer\n",
        "      QConv2d(first_layer_features, second_layer_features, kernel_size=3, stride=1, padding=1, wbit=wbit, abit=abit, sparsity_fraction=second_layer_sparsity),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "    # Defining fully-connected linear layer\n",
        "    self.linear_layers = nn.Sequential(\n",
        "      QLinear(second_layer_features * 7 * 7, 10)\n",
        "    )\n",
        "\n",
        "  # Defining the forward pass    \n",
        "  def forward(self, x):\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear_layers(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_qRpO3NL2CU"
      },
      "source": [
        "## Quantized Convolutional Layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7tA_pLItmjI"
      },
      "outputs": [],
      "source": [
        "# Defining quantization nodes\n",
        "class QSTE(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, scale):\n",
        "    # Quantization\n",
        "    x = x / scale\n",
        "    x = torch.round(x)\n",
        "    # Dequantization\n",
        "    xdeq = x * scale\n",
        "    return xdeq\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    return grad_output, None\n",
        "\n",
        "\n",
        "# Define your convolutional layer\n",
        "class QConv2d(nn.Conv2d):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None, wbit=4, abit=4, sparsity_fraction=0.333):\n",
        "      super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode, device=device, dtype=dtype)\n",
        "\n",
        "      self.register_buffer('mask', torch.ones(self.weight.size()))\n",
        "      self.wbit = wbit\n",
        "      self.abit = abit\n",
        "      self.sparsity_fraction = sparsity_fraction\n",
        "      global sparsity\n",
        "\n",
        "  # Weight Quantization\n",
        "  def wquant(self):\n",
        "    # Defining quantization boundaries\n",
        "    self.alpha_w = 2*self.weight.abs().mean()\n",
        "    wc = self.weight.clamp(-self.alpha_w, self.alpha_w)\n",
        "    # Calculating scaling factor\n",
        "    scaling_factor = (self.alpha_w) / (2**(self.wbit-1)-1)\n",
        "    # step 2: quantization\n",
        "    wq = QSTE.apply(wc, scaling_factor)\n",
        "    return wq\n",
        "  \n",
        "  # Input Quantization\n",
        "  def xquant(self, x):\n",
        "    # Defining quantization boundaries: \n",
        "    self.alpha_w_upper = 6\n",
        "    self.alpha_w_lower = 0\n",
        "    xc = x.clamp(self.alpha_w_lower, self.alpha_w_upper)\n",
        "    # Calculating scaling factor\n",
        "    scaling_factor = (self.alpha_w_upper - self.alpha_w_lower) / (2**self.abit-1)\n",
        "    # step 2: quantization\n",
        "    xq = QSTE.apply(xc, scaling_factor)\n",
        "    return xq\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Quantizing weights\n",
        "    wq = self.wquant()\n",
        "    # Quantizing inputs\n",
        "    xq = self.xquant(x)\n",
        "    # Generating mask for pruning weights\n",
        "    mask = prune_net(wq, sparsity=self.sparsity_fraction)\n",
        "    reset = torch.ones_like(wq)\n",
        "    mask = mask[:, None, None, None].mul(reset)\n",
        "    wq_masked = wq.mul(mask)\n",
        "    self.mask = mask\n",
        "    # Running PyTorch's convolution function with quantized values\n",
        "    Y = F.conv2d(xq, wq.mul(self.mask), self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u4__amfGL3h"
      },
      "source": [
        "## Quantized Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KL4Vpsk1F8ZX"
      },
      "outputs": [],
      "source": [
        "# Defining the linear layer\n",
        "class QLinear(nn.Linear):\n",
        "  def __init__(self, in_channels, out_channels, bias=False, wbit=4, abit=4):\n",
        "      super().__init__(in_channels, out_channels, bias=bias)\n",
        "      self.wbit = wbit\n",
        "      self.abit = abit\n",
        "\n",
        "  # Weight quantization\n",
        "  def wquant(self):\n",
        "    # Setting quantization boundary: \n",
        "    self.alpha_w = 2*self.weight.abs().mean()\n",
        "    wc = self.weight.clamp(-self.alpha_w, self.alpha_w)\n",
        "    # Calculating scaling factor\n",
        "    scaling_factor = (self.alpha_w) / (2**(self.wbit-1)-1)\n",
        "    # Weight quantization\n",
        "    wq = QSTE.apply(wc, scaling_factor)\n",
        "    return wq\n",
        "\n",
        "  # Input quantization\n",
        "  def xquant(self, x):\n",
        "    # Defining quantization boundaries\n",
        "    self.alpha_w_upper = 6\n",
        "    self.alpha_w_lower = 0\n",
        "    xc = x.clamp(self.alpha_w_lower, self.alpha_w_upper)\n",
        "    # Calculating scaling factor\n",
        "    scaling_factor = (self.alpha_w_upper - self.alpha_w_lower) / (2**self.abit-1)\n",
        "    # Input quantization\n",
        "    xq = QSTE.apply(xc, scaling_factor)\n",
        "    return xq\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Quantizing weights\n",
        "    wq = self.wquant()\n",
        "    # Quantizing inputs\n",
        "    xq = self.xquant(x)\n",
        "    # Running PyTorch's linear function with quantized values\n",
        "    Y = F.linear(xq, wq, self.bias)\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvl1AguHU8YX"
      },
      "source": [
        "## Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLE2_HMyU75g"
      },
      "outputs": [],
      "source": [
        "# Code for pruning\n",
        "def prune_net(W, sparsity):\n",
        "  W1 = W.clone()\n",
        "  wg = W1.contiguous().view(W1.size(0), W1.size(1)*W1.size(2)*W1.size(3))\n",
        "  # Step2: Compute the norm of each group\n",
        "  wnorm = wg.norm(dim=1)\n",
        "  # Step 3: Sort the score and remove lowest\n",
        "  wn_sorted, indx = torch.sort(wnorm)\n",
        "  # Step 4: Given the sparsity, remove the group\n",
        "  thres_index = int(sparsity * len(wn_sorted)) # Finding threshold index, given sparsity\n",
        "  threshold = wn_sorted[(thres_index-1)] if thres_index != 0 else wn_sorted[(thres_index)]  # Finding threshold\n",
        "  mask = wnorm.gt(threshold).float() # Generating mask\n",
        "  return mask\n",
        "\n",
        "# Use this for a variable sparsity variation scheduler\n",
        "def get_sparsity(t, n):\n",
        "  si = 0\n",
        "  sf = 0.5\n",
        "  t0 = 0\n",
        "  del_t = 1\n",
        "  st = sf + (si-sf)*((1-((t-t0)/(n*del_t)))*(1-((t-t0)/(n*del_t)))*(1-((t-t0)/(n*del_t))))\n",
        "  global sparsity\n",
        "  sparsity = st\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H7yR0fzVA0a"
      },
      "source": [
        "# Main Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q0Rq-K8kzCaC",
        "outputId": "a593cb1e-bfc6-4edf-91a1-50acf7a0a8ed"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "if __name__ == '__main__':\n",
        "  batch = 128\n",
        "  epoch = 100\n",
        "  learn_rate = 0.005\n",
        "  momentum = 0.9\n",
        "  # Obtaining dataset\n",
        "  trainset, validset, testset = data_processing(cnn=True)\n",
        "\n",
        "  #========= Convolutional Neural Network =========\n",
        "  set_seed()\n",
        "  model = CNN()\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "  nn_run(model, trainset, validset, testset, batch, learn_rate, epoch, momentum, \"CNN\", decay=1e-4)\n",
        "  # Saving model to memory\n",
        "  torch.save(model.state_dict(), \"trained_network.pth.tar\")\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Neuromorphic_Project_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

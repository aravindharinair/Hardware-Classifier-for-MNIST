{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TZLn8G0NxA2",
        "outputId": "b8318f62-5bce-45f5-ebfd-3a1c8ac2817e"
      },
      "outputs": [],
      "source": [
        "import pickle as cPickle, gzip, numpy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as td\n",
        "import torch.nn.functional as F\n",
        "import time, random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple\n",
        "from torch import nn\n",
        "from torch.utils.data.dataset import TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# COLAB _SPECIFIC CODE\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# try:\n",
        "#   from fxpmath import Fxp\n",
        "# except:\n",
        "#   !pip install fxpmath\n",
        "from fxpmath import Fxp\n",
        "\n",
        "# Setting Random Seed\n",
        "manualSeed = 5000\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def set_seed():\n",
        "  if device == 'cuda':\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.enabled = False \n",
        "  torch.manual_seed(manualSeed)\n",
        "  np.random.seed(manualSeed)\n",
        "  random.seed(manualSeed)\n",
        "\n",
        "sparsity = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnC15t9KP_OJ"
      },
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u1o8JU1yP4TD"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "def data_processing(cnn):\n",
        "  # f = gzip.open('/content/drive/MyDrive/Sem 2 (Spring 2022)/EEE 598/Homeworks/HW2/mnist.pkl.gz', 'rb')\n",
        "  f = gzip.open('mnist.pkl.gz', 'rb')\n",
        "  train_set, valid_set, test_set = cPickle.load(f, encoding='latin1') \n",
        "  f.close()\n",
        "  # Converting dataset from numpy to Tensor\n",
        "  train_t = torch.from_numpy(train_set[0])\n",
        "  valid_t = torch.from_numpy(valid_set[0])\n",
        "  test_t  = torch.from_numpy(test_set[0])\n",
        "  # Converting labels to Tensor\n",
        "  train_label= torch.from_numpy(train_set[1])\n",
        "  valid_label= torch.from_numpy(valid_set[1])\n",
        "  test_label = torch.from_numpy(test_set[1])\n",
        "  if cnn:\n",
        "    # resize and normalize\n",
        "    train_t = train_t.reshape((train_t.shape[0], 1, 28, 28))\n",
        "    valid_t = valid_t.reshape((valid_t.shape[0], 1, 28, 28))\n",
        "    test_t = test_t.reshape((test_t.shape[0], 1, 28, 28))\n",
        "    input_shape = (1, 28, 28)\n",
        "  # Wrapping it\n",
        "  trainset = TensorDataset(train_t, train_label)\n",
        "  validset = TensorDataset(valid_t, valid_label)\n",
        "  testset = TensorDataset(test_t, test_label)\n",
        "\n",
        "  return trainset, validset, testset\n",
        "\n",
        "def loss_curve(train_loss, val_loss, y_range, name):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(name)\n",
        "  plt.plot(train_loss, label=\"Training Loss\")\n",
        "  plt.plot(val_loss, label=\"Validation Loss\")\n",
        "  plt.legend()\n",
        "  plt.savefig(name+\".png\", bbox_inches='tight', pad_inches=0.1)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl1hXSojQPoQ"
      },
      "source": [
        "## Accuracy Calculator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ezaKw4geQLwm"
      },
      "outputs": [],
      "source": [
        "# Accuracy Calculator\n",
        "def acc_calc(pred, actual):\n",
        "  best = pred.argmax(1)\n",
        "  comp = best.eq(actual.view_as(best)).float().sum()\n",
        "  return comp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gbKTFNNQdvs"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tKuqeMrMQcqq"
      },
      "outputs": [],
      "source": [
        "# Function for validation and testing\n",
        "def eval_loop(data_loader, model, loss_fn):\n",
        "  size = len(data_loader.dataset)\n",
        "  loss, acc, loss_val = 0, 0, 0\n",
        "  model.eval()\n",
        "  torch.no_grad() \n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    if torch.cuda.is_available():\n",
        "      X = X.cuda()\n",
        "      y = y.cuda()\n",
        "    \n",
        "    target = model(X)\n",
        "    loss = loss_fn(target, y).item()\n",
        "    # Accuracy\n",
        "    acc = acc+acc_calc(nn.Softmax(dim=1)(target), y)\n",
        "    loss_val = loss_val+loss\n",
        "\n",
        "  return loss_val/size, (acc/size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gdAbDxI3QoBR"
      },
      "outputs": [],
      "source": [
        "# Primary function for execution of the neural networks\n",
        "def nn_run(nn_model, trainset, validset, testset, batch, learn_rate, epoch_max, mom, model, decay):\n",
        "  \n",
        "  # Loading datasets\n",
        "  train_loader = td.DataLoader(trainset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "  valid_loader = td.DataLoader(validset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "  test_loader  = td.DataLoader(testset, batch_size=batch, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "\n",
        "  # Loading neural network model to device\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(nn_model.parameters(), lr=learn_rate, momentum=mom, weight_decay=decay)\n",
        "  # optimizer = torch.optim.SGD(nn_model.parameters(), lr=learn_rate, weight_decay=decay)\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
        "\n",
        "  print(f'\\n=============={model}==============')\n",
        "  print(f'Trained model loaded.')\n",
        "  timer = time.perf_counter()\n",
        "  valid_loss = 1\n",
        "  valid_loss_list = []\n",
        "  train_loss_list = []\n",
        "\n",
        "  #=========TESTING=========\n",
        "  print(f'\\nTesting using {device} device.')\n",
        "  timer = time.perf_counter()\n",
        "  test_loss, test_acc = eval_loop(test_loader, nn_model, loss_fn)\n",
        "  timer = round(time.perf_counter()-timer, 2)\n",
        "\n",
        "  print('--------Test complete.--------')\n",
        "  print(f'Model       : {model}')\n",
        "  print(f'Device      : {device}')\n",
        "  print(f'Loss        : {test_loss:>7f}')\n",
        "  print(f'Accuracy    : {np.round(test_acc.cpu().detach().numpy()*100, 3)}%') if device == 'cuda' else print(f'Accuracy    : {np.round(test_acc.detach().numpy()*100, 3)}%')\n",
        "  print(f'Runtime     : {timer}s')\n",
        "  print(f'Dataset size: {len(test_loader.dataset)}')\n",
        "  print('------------------------------\\n')\n",
        "\n",
        "  # Returning test accuracy to main function\n",
        "  return np.round(test_acc.cpu().detach().numpy()*100, 3) if device == 'cuda' else np.round(test_acc.detach().numpy()*100, 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3s9D8JOUckx"
      },
      "source": [
        "## Neural Network Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2qmErQN2UZ2n"
      },
      "outputs": [],
      "source": [
        "# Defining the convolutional neural network (CNN)\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    first_layer_features = 16\n",
        "    second_layer_features = 32\n",
        "    wbit = 4\n",
        "    abit = 4\n",
        "    self.cnn_layers = nn.Sequential(\n",
        "      # Defining a 2D convolution layer\n",
        "      QConv2d(1, first_layer_features, kernel_size=3, stride=1, padding=1, wbit=wbit, abit=abit, scaling_next=1, layer=1, sparsity_fraction=0.333),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "      # Defining another 2D convolution layer\n",
        "      QConv2d(first_layer_features, second_layer_features, kernel_size=3, stride=1, padding=1, wbit=wbit, abit=abit, scaling_next=1, layer=2, sparsity_fraction=0.5),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "    )\n",
        "\n",
        "    self.linear_layers = nn.Sequential(\n",
        "      QLinear(second_layer_features * 7 * 7, 10)\n",
        "    )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "  def forward(self, x):\n",
        "    x = self.cnn_layers(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear_layers(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIb3rphLU0KO"
      },
      "source": [
        "## Quantized Convolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ulMi8B5AUzlB"
      },
      "outputs": [],
      "source": [
        "# Step 0. Define your quantization backward node (function)\n",
        "import logging    # first of all import the module\n",
        "logging.basicConfig(filename='std.log', filemode='w')\n",
        "\n",
        "class QSTE(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx, x, scale):\n",
        "    # quantization - dequantization\n",
        "    x = x / scale\n",
        "    x = torch.round(x)\n",
        "\n",
        "    xdeq = x * scale\n",
        "    return xdeq\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    return grad_output, None\n",
        "\n",
        "\n",
        "# Step 1. Define your convolutional layer\n",
        "class QConv2d(nn.Conv2d):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros', device=None, dtype=None, wbit=4, abit=4, scaling_next=1, layer=1, sparsity_fraction=0):\n",
        "      super().__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode, device=device, dtype=dtype)\n",
        "\n",
        "      self.register_buffer('mask', torch.ones(self.weight.size()))\n",
        "      self.register_buffer('wint', torch.ones(self.weight.size()))\n",
        "      self.register_buffer('scale', torch.tensor(1.))      \n",
        "      self.wbit = wbit\n",
        "      self.abit = abit\n",
        "      self.scaling_next = scaling_next\n",
        "      self.layer = layer\n",
        "      self.sparsity_fraction = sparsity_fraction\n",
        "      global sparsity\n",
        "\n",
        "  def wquant(self):\n",
        "    \"\"\"\n",
        "    Perform the weight quantization\n",
        "    \"\"\"\n",
        "    # step 1: quantization boundary: \n",
        "    self.alpha_w = 2*self.weight.abs().mean()\n",
        "    wc = self.weight.clamp(-self.alpha_w, self.alpha_w)\n",
        "    scaling_factor = (self.alpha_w) / (2**(self.wbit-1)-1)\n",
        "    # step 2: quantization\n",
        "    wc = wc / scaling_factor\n",
        "    wq = torch.round(wc)\n",
        "    #wq = QSTE.apply(wc, scaling_factor)\n",
        "    return wq, scaling_factor\n",
        "  \n",
        "  def xquant(self, x):\n",
        "    \"\"\"\n",
        "    Perform the wight quantization\n",
        "    \"\"\"\n",
        "    # step 1: quantization boundary: \n",
        "    self.alpha_w_upper = 6\n",
        "    self.alpha_w_lower = 0\n",
        "    xc = x.clamp(self.alpha_w_lower, self.alpha_w_upper)\n",
        "    scaling_factor = (self.alpha_w_upper - self.alpha_w_lower) / (2**self.abit-1)\n",
        "    # step 2: quantization\n",
        "    xc = xc / scaling_factor\n",
        "    xq = torch.round(xc)\n",
        "    #xq = QSTE.apply(xc, scaling_factor)\n",
        "    return xq, scaling_factor\n",
        "\n",
        "  def forward(self, x):\n",
        "    # step 1: Quantize your weights\n",
        "    wq, Sw1 = self.wquant()\n",
        "    # map the integer weights to the buffer\n",
        "    self.wint.data = wq.mul(self.mask)\n",
        "    # step 2: Quantize your input x\n",
        "    if wq.size(1) == 1:\n",
        "      xq, Sx1 = self.xquant(x)\n",
        "    else:\n",
        "      xq = x\n",
        "      Sx1 = 6 / (2**self.abit-1)\n",
        "    M = Sw1 * Sx1 / Sx1\n",
        "    # use Fxp to do conversion of M \n",
        "    M_scalar = Fxp(M.detach().cpu().numpy(), signed=True, n_word=16, n_frac=12)\n",
        "    Mint = M_scalar.base_repr(10)\n",
        "    Mint = int(Mint)\n",
        "    self.scale = torch.tensor(Mint)\n",
        "    # self.wint = wq.mul(self.mask)\n",
        "    Y = F.conv2d(xq, wq.mul(self.mask), self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "    Y = Y.mul(Mint).mul(2**(-12)).round()\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tguU87U6U7Nt"
      },
      "source": [
        "## Quantized Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XEVAjLsdU6uu"
      },
      "outputs": [],
      "source": [
        "# Step 1. Define your convolutional layer\n",
        "class QLinear(nn.Linear):\n",
        "  def __init__(self, in_channels, out_channels, bias=False, wbit=4, abit=4):\n",
        "      super().__init__(in_channels, out_channels, bias=bias)\n",
        "\n",
        "      #self.register_buffer('mask', torch.ones(self.weight.size()))\n",
        "      self.register_buffer('wint', torch.ones(self.weight.size()))\n",
        "      self.register_buffer('scale', torch.tensor(1.))      \n",
        "      self.wbit = wbit\n",
        "      self.abit = abit\n",
        "      #global sparsity\n",
        "\n",
        "  \n",
        "  def wquant(self):\n",
        "    \"\"\"\n",
        "    Perform the weight quantization\n",
        "    \"\"\"\n",
        "    # step 1: quantization boundary: \n",
        "    self.alpha_w = 2*self.weight.abs().mean()\n",
        "    wc = self.weight.clamp(-self.alpha_w, self.alpha_w)\n",
        "    scaling_factor = (self.alpha_w) / (2**(self.wbit-1)-1)\n",
        "    # step 2: quantization\n",
        "    wc = wc / scaling_factor\n",
        "    wq = torch.round(wc)\n",
        "    #wq = QSTE.apply(wc, scaling_factor)\n",
        "    return wq, scaling_factor\n",
        "  \n",
        "  def xquant(self, x):\n",
        "    \"\"\"\n",
        "    Perform the wight quantization\n",
        "    \"\"\"\n",
        "    # step 1: quantization boundary: \n",
        "    self.alpha_w_upper = 6\n",
        "    self.alpha_w_lower = 0\n",
        "    xc = x.clamp(self.alpha_w_lower, self.alpha_w_upper)\n",
        "    scaling_factor = (self.alpha_w_upper - self.alpha_w_lower) / (2**self.abit-1)\n",
        "    # step 2: quantization\n",
        "    xc = xc / scaling_factor\n",
        "    xq = torch.round(xc)\n",
        "    #xq = QSTE.apply(xc, scaling_factor)\n",
        "    return xq, scaling_factor\n",
        "\n",
        "  def forward(self, x):\n",
        "    # step 1: Quantize your weights\n",
        "    wq, Sw1 = self.wquant()\n",
        "    # step 2: Quantize your input x\n",
        "    xq = x\n",
        "    Sx1 = 6 / (2**self.abit-1)\n",
        "\n",
        "    Y = F.linear(xq, wq, self.bias)\n",
        "    M = Sw1 * Sx1\n",
        "    M_scalar = Fxp(M.detach().cpu().numpy(), signed=True, n_word=16, n_frac=12)\n",
        "    Mint = M_scalar.base_repr(10)\n",
        "    Mint = int(Mint)\n",
        "    self.scale = torch.tensor(Mint)\n",
        "\n",
        "\n",
        "    Y = Y.mul(Mint).mul(2**(-12)).round()\n",
        "    return Y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOVScmZIVU2A"
      },
      "source": [
        "## Main function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "iKWELcabVUUs",
        "outputId": "2ea5c4e8-6ba6-4cd1-a08a-3ded72c28bca"
      },
      "outputs": [],
      "source": [
        "# Main Function\n",
        "import os\n",
        "if __name__ == '__main__':\n",
        "  batch = 128\n",
        "  epoch = 100\n",
        "  learn_rate = 0.005\n",
        "  momentum = 0.9\n",
        "  # Obtaining dataset\n",
        "  trainset, validset, testset = data_processing(cnn=True)\n",
        "\n",
        "  #========= Convolutional Neural Network =========\n",
        "  set_seed()\n",
        "  ckpt = torch.load('trained_network.pth.tar')\n",
        "  print(os.path.abspath('trained_network.pth.tar'))\n",
        "  model = CNN()\n",
        "\n",
        "  curr_state_dict = model.state_dict()\n",
        "  curr_state_dict.update(ckpt)\n",
        "  model.load_state_dict(curr_state_dict)\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "  nn_run(model, trainset, validset, testset, batch, learn_rate, epoch, momentum, \"CNN\", decay=1e-4)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3peauBKE3lU",
        "outputId": "a9470f73-a0fb-413c-8e68-09c8b260ab9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['cnn_layers.0.weight', 'cnn_layers.0.mask', 'cnn_layers.0.wint', 'cnn_layers.0.scale', 'cnn_layers.3.weight', 'cnn_layers.3.mask', 'cnn_layers.3.wint', 'cnn_layers.3.scale', 'linear_layers.0.weight', 'linear_layers.0.wint', 'linear_layers.0.scale'])\n"
          ]
        }
      ],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict.keys())\n",
        "for k, v in state_dict.items():\n",
        "  if 'wint' in k:\n",
        "    if device == 'cuda':\n",
        "      wint = v.cpu().numpy()\n",
        "    else:\n",
        "      wint = v.numpy()\n",
        "    np.save(f\"{k}_wint.npy\", wint)\n",
        "  elif 'scale' in k:\n",
        "    scale_factor = v.numpy()\n",
        "    np.save(f\"{k}_scale.npy\", scale_factor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZtboBT3oFq6L"
      },
      "outputs": [],
      "source": [
        "wnp = np.load(\"cnn_layers.0.wint_wint.npy\")\n",
        "f = open(\"cnn_layers.0.wint.txt\", 'w')\n",
        "for x in wnp:\n",
        "  if(np.any(x)):\n",
        "    f.write(np.array2string(x))\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "wnp = np.load(\"cnn_layers.3.wint_wint.npy\")\n",
        "f = open(\"cnn_layers.3.wint.txt\", 'w')\n",
        "for x in wnp:\n",
        "  if(np.any(x)):\n",
        "    f.write(np.array2string(x))\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "wnp = np.load(\"linear_layers.0.wint_wint.npy\")\n",
        "f = open(\"linear_layers.0.wint.txt\", 'w')\n",
        "for x in wnp:\n",
        "  if(np.any(x)):\n",
        "    f.write(np.array2string(x))\n",
        "    f.write(\"\\n\")\n",
        "f.close()\n",
        "\n",
        "f = open(\"scaling_factors.txt\", 'w')\n",
        "wnp = np.load(\"cnn_layers.0.scale_scale.npy\", allow_pickle=True)\n",
        "f.write(\"\\nScaling factor conv layer 0: \")\n",
        "f.write(np.array2string(wnp))\n",
        "wnp = np.load(\"cnn_layers.3.scale_scale.npy\", allow_pickle=True)\n",
        "f.write(\"\\nScaling factor conv layer 3: \")\n",
        "f.write(np.array2string(wnp))\n",
        "wnp = np.load(\"linear_layers.0.scale_scale.npy\", allow_pickle=True)\n",
        "f.write(\"\\nScaling factor linear layer 0: \")\n",
        "f.write(np.array2string(wnp))\n",
        "f.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlu1pA8IF017",
        "outputId": "1ab643ef-7fee-482f-da65-42b846612f12"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:549: UserWarning: non-inplace resize is deprecated\n",
            "  warnings.warn(\"non-inplace resize is deprecated\")\n"
          ]
        }
      ],
      "source": [
        "def xquant_temp(x):\n",
        "  \"\"\"\n",
        "  Perform the weight quantization\n",
        "  \"\"\"\n",
        "  # step 1: quantization boundary: \n",
        "  alpha_w_upper = 6\n",
        "  alpha_w_lower = 0\n",
        "  xc = x.clamp(alpha_w_lower, alpha_w_upper)\n",
        "  scaling_factor = (alpha_w_upper - alpha_w_lower) / (2**4-1)\n",
        "  # step 2: quantization\n",
        "  xc = xc / scaling_factor\n",
        "  xq = torch.round(xc)\n",
        "  #xq = QSTE.apply(xc, scaling_factor)\n",
        "  return xq\n",
        "\n",
        "from torch.functional import Tensor\n",
        "test_loader  = td.DataLoader(testset, batch_size=1, shuffle=True, worker_init_fn=np.random.seed(manualSeed),num_workers=0,pin_memory=True)\n",
        "len(test_loader.dataset)\n",
        "f = open(\"input_fmap.txt\", 'w')\n",
        "for batch, (X, y) in enumerate(test_loader):\n",
        "  X = X .resize(28,28)\n",
        "  X = xquant_temp(X)\n",
        "  X = X.numpy()\n",
        "  for pixel in X:\n",
        "    f.write(np.array2string(pixel))\n",
        "  break\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSnX7I7vF50n"
      },
      "source": [
        "# DEBUG CODES BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAYikyakb3AL",
        "outputId": "90c400ee-593e-4406-ddd8-8ca4541f6038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity of the layer cnn_layers.0= 0.104\n",
            "Sparsity of the layer cnn_layers.3= 0.516\n"
          ]
        }
      ],
      "source": [
        "# TESTING\n",
        "for n, m in model.named_modules():\n",
        "  if isinstance(m, QConv2d):\n",
        "    wint = m.wint\n",
        "    s = wint[wint.eq(0.)].numel()/wint.numel()\n",
        "    print(\"Sparsity of the layer {}= {:.3f}\".format(n, s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvC7jxzSnBsw",
        "outputId": "d457fc37-e126-4d72-8337-98e1b228ee02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['cnn_layers.0.weight', 'cnn_layers.0.mask', 'cnn_layers.0.wint', 'cnn_layers.0.scale', 'cnn_layers.3.weight', 'cnn_layers.3.mask', 'cnn_layers.3.wint', 'cnn_layers.3.scale', 'linear_layers.0.weight', 'linear_layers.0.wint', 'linear_layers.0.scale'])\n",
            "Sparsity of the layer cnn_layers.0.wint= 0.104\n",
            "Sparsity of the layer cnn_layers.3.wint= 0.516\n",
            "Sparsity of the layer linear_layers.0.wint= 0.000\n"
          ]
        }
      ],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict.keys())\n",
        "for k, v in state_dict.items():\n",
        "  if 'wint' in k:\n",
        "    wint = v\n",
        "    s = wint[wint.eq(0.)].numel()/wint.numel()\n",
        "    print(\"Sparsity of the layer {}= {:.3f}\".format(k, s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFVLKGp2nTXv",
        "outputId": "d61d265e-1c00-40aa-e1a3-7de044b0cc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 16, 3, 3)\n"
          ]
        }
      ],
      "source": [
        "wnp = np.load(\"cnn_layers.3.wint_wint.npy\")\n",
        "print(wnp.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJd8eGZWTLB9"
      },
      "source": [
        "energy calc = power * time \n",
        "is time just cycles or period"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
